Code Review and AIâ€“MCP Integration Plan
1Â Repository Overview and Current State

The newspaper repository targets an AIâ€‘oriented news aggregation pipeline. At its core it provides:

Data model (db/schema_v2.sql) â€“ defines a documentâ€‘centric schema with tables for documents, chunks, vector embeddings, entities, events and hints. Documents hold basic metadata and raw payload; chunks index segments of text; vector embeddings capture semantic information in a specified space (e.g., bgeâ€‘m3) using the vector(768) type and support both L2 and cosine HNSW indexes
GitHub
. Entities, mentions and events provide hooks for knowledge extraction, with eventâ€evidence linking via a manyâ€‘toâ€‘many table.

Ingestion scripts â€“ scripts/ingest_rss.py reads RSS/Atom feeds and populates the doc and chunk tables. It canonicalises URLs to avoid duplicates, stores language and author when available, and upserts hints like genre_hint
GitHub
. Similar scripts exist for NewsAPI and Hacker News. Ingestion is decoupled from embedding and server execution via systemd services and timers.

Embedding script â€“ scripts/embed_chunks.py loads a SentenceTransformer model, fetches unâ€‘embedded chunks, encodes them with optional normalisation, and writes the results into chunk_vec
GitHub
. The script registers pgvector for psycopg and ensures timezone consistency.

Stubs for entity and event extraction â€“ scripts/entity_link_stub.py implements a placeholder entity linker that tokenises simple English words and writes token IDs to the entity and mention tables
GitHub
. scripts/event_extract_stub.py produces dummy events for chunks without evidence
GitHub
. These are minimal but provide integration points for later NLP modules.

MCP server â€“ mcp_news/server.py wraps the FastMCP framework and exposes tools (doc_head, semantic_search, entity_search, event_timeline). It lazily loads a SentenceTransformer based on ENABLE_SERVER_EMBEDDING. Semantic search first tries vector search using <=> (cosine distance) and falls back to recency when embeddings are missing
GitHub
. Results are normalised into a Bundle typed dict. Event timeline queries allow filtering by entity ID, type or time range
GitHub
.

Systemd deployment â€“ deploy/* includes service and timer units for ingestion and embedding, with environment variables in /etc/default/mcp-news controlling DB and API keys. The DayÂ 1 report describes an Ubuntuâ€‘based deployment and emphasises PGVector, systemd timers and CI checks
GitHub
.

Documentation and CI â€“ the repository contains detailed reports explaining design decisions (e.g., migrating to cosine distance and HNSW indexes
GitHub
) and notes the need to adjust dimension when changing models
GitHub
. Tests cover the stub functions. Continuous integration installs dependencies, applies the schema and runs the test suite.

2Â Code Review and Observations
2.1Â Schema

The doc table normalises articles by canonical URL and stores both the raw payload and a preâ€‘computed body hash for duplicate detection
GitHub
. Indexes on published_at and url_canon are already present, but adding a composite index on (source,published_at DESC) would speed up perâ€‘source queries.

Vector embeddings are stored in chunk_vec.emb with a fixed dimension of 768 and HNSW indexes for both L2 and cosine distance
GitHub
. This design supports efficient approximate nearest neighbour search but requires reâ€‘indexing if switching models with different embedding sizes.

Entities and events are kept simple; event times and geohashes are nullable to allow stub extraction. The hint table stores lightweight metadata such as IPTC genre codes. Additional indexes on hint (key) and hint (doc_id) are present
GitHub
 to speed up lookups.

2.2Â Ingestion and Embedding Scripts

The RSS ingester normalises URLs and gracefully handles date parsing errors by falling back to the current time
GitHub
. It uses upsert semantics to avoid duplicates based on url_canon. However, it updates only the title on conflict; other fields such as raw or author might differ on subsequent runs and should also be considered for updating.

Ingested documents are broken into a single chunk containing the summary or title. This simplifies embedding but limits recall when searching full articles. Future iterations could add content extraction (e.g., boilerplate removal) and tokenise long articles into multiple overlapping windows to improve semantic retrieval.

The embedding script encodes small batches and writes them in a single transaction. It registers the vector extension for psycopg and normalises embeddings for cosine compatibility. This is efficient, but error handling could be improved by logging exceptions and continuing rather than exiting silently.

2.3Â MCP Server

The FastMCP wrapper exposes tools for retrieving document heads and performing semantic search. The semantic_search function first attempts an embeddingâ€‘based query using <=> and falls back to recency ordering when embeddings are missing
GitHub
. It checks the ENABLE_SERVER_EMBEDDING environment variable to control whether the model is loaded and uses pgvector.psycopg.Vector to ensure type matching
GitHub
. This design gracefully handles missing models or vector indexes.

event_timeline builds flexible SQL conditions based on optional filters (type_id, ext_id, time range) and returns events with ISOâ€‘formatted timestamps
GitHub
. For large datasets, pagination parameters (limit/offset) could be added.

The server file uses typing_extensions.TypedDict to maintain compatibility with Pydantic under PythonÂ 3.11
GitHub
.

2.4Â Stub Modules

The entity linker stub tokenises capitalised alphanumeric words and treats them as generic tokens
GitHub
. It upserts these into the entity table with a tok: prefix and writes mentions with a fixed confidence ofÂ 0.1
GitHub
. This demonstrates the pipeline but offers no real NER capability; integration of a proper multilingual NER (spaCy, Stanza or SudachiPy) and candidate generation via Wikidata/Geonames APIs is planned.

The event extraction stub simply creates an event for every unprocessed chunk and links evidence
GitHub
. While sufficient for testing the event schema, it yields meaningless events. A true event extractor would require relation extraction and temporal normalisation (e.g., SRL/OIE plus Heideltime or SUTime) and maybe geocoding.

2.5Â Web UI (Temporary)

The minimal FastAPI/Jinja interface added in your last patch provides an HTML list of recent documents, search by ILIKE, filters and pagination. This UI is deliberately simple and can be discarded or kept for internal testing. More robust userâ€‘facing interfaces should likely live in a separate repository or Next.js frontend and query the MCP server via HTTP.

3Â Analysis and Recommendations for Completion
3.1Â Finish the AIâ€‘Native Pipeline

The longâ€‘term goal is an AI system that can retrieve relevant news from the MCP server and perform deeper analysis (summarisation, trend detection, question answering). To reach this goal:

Improve content ingestion â€“ move beyond summaries: implement content extraction (e.g., readability-lxml or newspaper3k) and split long articles into multiple chunks (e.g., 512â€‘token windows with overlap). This will increase recall for semantic search and provide richer context for downstream models.

Entity linking and event extraction â€“ replace the stubs with real NLP components. Suggested pipeline:

Use a multilingual NER library (spaCy with Japanese models, Stanza or SudachiPy) to extract named entities from each chunk.

Query Wikidata or other KBs to map entity strings to ext_id using candidate selection and ranking (e.g., using BM25 over alias tables). Cache results to reduce API calls.

For event extraction, adopt either a templateâ€‘based approach (detecting â€œX said Yâ€, â€œA bought Bâ€) or an open information extraction model. Use tools like PropBank/Semantic Role Labelling or custom BERT classifiers to identify event type and participants.

Normalise times using a temporal tagger and generate geohashes from place names.

Semantic search and ranking fusion â€“ the repository already supports vector search using PGVectorâ€™s <=> operator and falling back to recency. The next step is to combine multiple signals: lexical matching (e.g., PGTrgm or BM25), semantic similarity, recency, source reliability and user preferences. A simple linear combination of normalised scores can be implemented in SQL or Python, with weights tuned on a heldâ€‘out set. For Japanese search, PGroonga or Meilisearch could improve morphological indexing.

AI orchestration â€“ implement an application layer where an LLM can call MCP tools autonomously. For example, use a LangChain or FastMCP agent that decides whether to call semantic_search, entity_search or event_timeline based on the query and then summarises the returned articles. The server already defines these tools, so the agent can be built on top of them. Caching and rate limiting should be added to avoid expensive embedding reâ€‘computations.

Monitoring and evaluation â€“ integrate Prometheus metrics for ingestion lag, embedding throughput and search latency. Develop an evaluation script that computes recall/nDCG on a curated question set with groundâ€‘truth relevant documents (e.g., the eval_recall_stub.py hints at this). Use these metrics to guide model selection and ranking weights.

3.2Â Customising News Sources and Feeds

Flexible feed configuration â€“ define a declarative feeds.json where each entry includes URL, humanâ€‘readable name, genre code, language, and optional extraction rules (CSS selectors for content extraction). Provide CLI flags or environment variables to override perâ€‘run options such as maximum entries and categories.

Adding new sources â€“ to support paywalled or proprietary feeds, implement connectors that call APIs (e.g., NewsAPI, GDELT, custom R&D feeds) or scrape websites ethically. Each connector should normalise output into the doc/chunk model and register its own systemd service and timer.

Perâ€‘source weighting and filtering â€“ store perâ€‘source reliability or priority weights in a source_meta table. Use these weights in ranking fusion and allow excluding lowâ€‘quality sources at query time. Genre hints could also be enriched via IPTC or custom taxonomies.

3.3Â Integration Plan and Timeline

A rough roadmap with estimated effort (assuming one engineer with NLP/ML skills):

Â Â Phase | Tasks | Est.Â Duration
Â Â --- | --- | ---
Â Â PhaseÂ 1Â (1â€“2Â weeks) | Productionise ingestion: add fullâ€‘text extraction for articles; refactor ingestion scripts into reusable modules; implement feed configuration file; add indexes (source,published_at) and trigram search for titles; remove the temporary FastAPI UI or replace it with a commandâ€‘line tool for sampling documents. | 1Â week
Â Â  | Semantic search tuning: evaluate different SentenceTransformer models (e.g., multilingualÂ E5 vs. BGE) and adjust embedding dimension. Implement a ranking fusion prototype combining vector similarity and recency/source weighting. | 1Â week
Â Â PhaseÂ 2Â (2â€“3Â weeks) | Entity linking: integrate an NER library and candidate selection from Wikidata/Geonames; store entity attributes (languageâ€‘agnostic names, categories). Evaluate precision on a small labeled set. | 1.5Â weeks
Â Â  | Event extraction: adopt an open information extraction or templateâ€‘based model to detect events (e.g., purchases, elections). Design an event schema aligned with schema.org or IPTC and populate event participants. | 1.5Â weeks
Â Â PhaseÂ 3Â (1â€“2Â weeks) | AI agent integration: build a LangChain or FastMCP agent that can call MCP tools, summarise results and answer user questions. Provide a simple API endpoint for the agent. Add caching and rate limiting. | 1Â week
Â Â  | Monitoring and evaluation: instrument ingestion/embedding processes with Prometheus; implement a test harness for recall/nDCG evaluation; tune ranking weights based on feedback. | 1Â week
Â Â Ongoing | Source expansion and customisation: continuously add new feeds and adjust weights; maintain connectors and handle API changes. Perform periodic data cleaning (duplicate detection, outdated news removal). | continuous

4Â Conclusion

The repository already lays a strong foundation for an AIâ€‘native news aggregator: a structured database schema, ingestion and embedding scripts, a semantic search server and systemd deployment scripts. To realise the vision of AIâ€‘driven news analysis, the next steps involve enhancing content extraction, implementing real entity and event extraction, improving search ranking and building an agent that can autonomously call MCP tools. Additionally, customising and weighting news sources will ensure better recall and user satisfaction. Following the proposed roadmap will help transform the current prototype into a robust platform where AI can access, analyse and summarise news in multiple languages.

æ—¥æœ¬èªç‰ˆ

# ã‚³ãƒ¼ãƒ‰ãƒ¬ãƒ“ãƒ¥ãƒ¼ & AIé€£æºè¨ˆç”»ï¼ˆç¿»è¨³ç‰ˆï¼‰

## 1. ç¾çŠ¶ãƒ¬ãƒ“ãƒ¥ãƒ¼ã®ã¾ã¨ã‚

### å…¨ä½“åƒ

* **DBã‚¹ã‚­ãƒ¼ãƒ**ã¯ `doc / chunk / chunk_vec / entity / event / hint` ã«åˆ†é›¢æ¸ˆã¿ã€‚AIæ¤œç´¢ã«æœ€é©ã€‚
* **ã‚µãƒ¼ãƒãƒ¼å´**ã¯ `mcp_news/server.py` ã«ã¦ `semantic_search` ã¨ `doc_head` ãŒMCPçµŒç”±ã§åˆ©ç”¨å¯èƒ½ã€‚
* **CI**ã¯GitHub Actionsã§Postgres(pgvectorå…¥ã‚Š)ã‚’èµ·å‹•â†’ã‚¹ã‚­ãƒ¼ãƒé©ç”¨â†’pytestã¾ã§ä¸€é€£ã§é€šã‚‹ã€‚
* **ã‚¹ã‚¿ãƒ–**ï¼ˆ`entity_link_stub.py`, `event_extract_stub.py`ï¼‰ãŒå½¢ã ã‘å­˜åœ¨ã—ã€ãƒ€ãƒŸãƒ¼å‹•ä½œã¯ç¢ºèªæ¸ˆã¿ã€‚

### æ”¹å–„ãƒã‚¤ãƒ³ãƒˆ

* ãƒ•ã‚¡ã‚¤ãƒ«ã«æ®‹ã£ã¦ã„ãŸ `...` ã‚„æœªå®Œæˆã‚³ãƒ¼ãƒ‰ã¯ä¿®æ­£æ¸ˆã¿ã ãŒã€ä¾ç„¶ã¨ã—ã¦ **Event/Entityå®Ÿè£…ã¯ç©º**ã€‚
* **è·é›¢é–¢æ•°ã®æ•´åˆæ€§**ï¼šãƒ™ã‚¯ãƒˆãƒ«ã¯cosæ­£è¦åŒ–ã€DBã¯L2ã‚ªãƒšãƒ¬ãƒ¼ã‚¿ã€ã©ã¡ã‚‰ã‹ã«çµ±ä¸€å¿…é ˆã€‚
* **ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä¸è¶³**ï¼š`doc.url_canon` ã‚„ `hint.key` ãªã©ã€å…¸å‹çš„ã«ä½¿ã†åˆ—ã«ç´¢å¼•è¿½åŠ æ¨å¥¨ã€‚
* **UI**ã¯ç¢ºèªç”¨ã«æœ€å°é™ã ã‘ã€‚å‰Šé™¤ã‚‚å¯ã€‚

---

## 2. è¨­è¨ˆæ–¹é‡ï¼ˆUbuntuç”¨ v2æ¡ˆã‚ˆã‚Šï¼‰

* **è¨€èªéä¾å­˜ãƒ»AIãƒ•ã‚¡ãƒ¼ã‚¹ãƒˆ**ï¼šæœ¬æ–‡ã¯åŸæ–‡ä¿å­˜ã€æ¤œç´¢ã¯å¤šè¨€èªå…±æœ‰ãƒ™ã‚¯ãƒˆãƒ«ç©ºé–“ï¼‹IDåŒ–ã•ã‚ŒãŸã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã§è§£æ±ºã€‚
* **MCPã¯æœ€å°JSON**ã‚’è¿”ã™ã®ã¿ï¼ˆ`title/published_at/genre_hint/url`ã€å¿…è¦ãªã‚‰entity QIDï¼‰ã€‚
* **ç¿»è¨³ã‚„UIå‘ã‘æ•´å½¢ã¯æœ€å¾Œ**ã€‚ä¿å­˜æ™‚ã«ã¯ç¿»è¨³ã‚’ä¸€åˆ‡ã—ãªã„ã€‚

### ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³

1. RSS/NewsAPI/HN ingest â†’ UTCãƒ»URLæ­£è¦åŒ–
2. ãƒãƒ£ãƒ³ã‚¯åŒ–ï¼ˆ1200Â±200æ–‡å­—ã€ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—200ï¼‰
3. ãƒ™ã‚¯ãƒˆãƒ«åŒ–ï¼ˆåŸ‹ã‚è¾¼ã¿ç©ºé–“ã”ã¨ã«HNSWç´¢å¼•ï¼‰
4. å®Ÿä½“ãƒªãƒ³ã‚¯ï¼ˆWikidata QIDãªã©ï¼‰
5. ã‚¤ãƒ™ãƒ³ãƒˆæŠ½è±¡åŒ–ï¼ˆèª°ãŒï¼ä½•ã‚’ï¼ã„ã¤ï¼ã©ã“ã§ï¼‰
6. é‡è¤‡æ’é™¤ï¼ˆURLï¼‹SimHashï¼‹ãƒ™ã‚¯ãƒˆãƒ«è¿‘å‚ï¼‰
7. MCPçµŒç”±ã§æ¤œç´¢å¿œç­”

---

## 3. æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ï¼ˆã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«æ¡ˆï¼‰

### ã‚¹ãƒ—ãƒªãƒ³ãƒˆ1ï¼ˆMVPå®Œæˆï¼‰

* RSSå–è¾¼ â†’ åŸ‹ã‚è¾¼ã¿ â†’ MCP `semantic_search`
* Recency fallbackç¢ºèªï¼ˆENABLE\_SERVER\_EMBEDDING=0ã§æ–°ç€é †ï¼‰
* `idx_doc_url`, `idx_hint_key` ãªã©æ±ç”¨ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹è¿½åŠ 

### ã‚¹ãƒ—ãƒªãƒ³ãƒˆ2ï¼ˆæ¤œç´¢å“è³ªå‘ä¸Šï¼‰

* NewsAPI/HN ingestå¾©æ—§
* ãƒ©ãƒ³ã‚¯èåˆï¼ˆcosé¡ä¼¼ Ã— æ–°é®®åº¦ Ã— ã‚½ãƒ¼ã‚¹ä¿¡é ¼åº¦ï¼‰
* systemd + Prometheusã§ç›£è¦–æŒ‡æ¨™åé›†ï¼ˆingestä»¶æ•°ã€vecé…å»¶ã€recall\@kï¼‰

### ã‚¹ãƒ—ãƒªãƒ³ãƒˆ3ï¼ˆAIãƒã‚¤ãƒ†ã‚£ãƒ–åŒ–ï¼‰

* å®Ÿä½“ãƒªãƒ³ã‚¯ï¼ˆNER+ELâ†’QIDï¼‰
* ã‚¤ãƒ™ãƒ³ãƒˆæŠ½è±¡åŒ–ï¼ˆSPOâ†’eventè¡¨ï¼‰
* è¿‘é‡è¤‡æ¤œçŸ¥ï¼ˆMinHash + Vecè¿‘å‚ï¼‰
* `entity_search` / `event_timeline` ã‚’MCPã«å…¬é–‹

---

## 4. ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚½ãƒ¼ã‚¹è¿½åŠ ãƒ»ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºæ¡ˆ

* **å…¬å¼RSSã¨APIå„ªå…ˆ**ï¼ˆNewsAPI, HN, å„ç¤¾RSSï¼‰
* **æ—¥æœ¬èªãƒ­ãƒ¼ã‚«ãƒ«åª’ä½“RSS**ã‚’è¿½åŠ ï¼ˆå…±åŒé€šä¿¡, æ—¥çµŒ, NHK, åœ°åŸŸç´™ï¼‰
* **ç‰¹æ®Šã‚¸ãƒ£ãƒ³ãƒ«**ï¼ˆã‚¹ãƒãƒ¼ãƒ„ãƒªãƒ¼ã‚°å…¬å¼, é‡‘èåºç™ºè¡¨, æ°—è±¡åºé€Ÿå ±ãªã©ï¼‰ã‚’ `genre_hint` ã¨çµ„ã¿åˆã‚ã›ã¦æ‹¡å¼µ
* **ä¿¡é ¼åº¦é‡ã¿ä»˜ã‘**ï¼ˆã‚½ãƒ¼ã‚¹åˆ¥ã«ä¿‚æ•°è¨­å®šï¼‰

---

## 5. AIã¨MCPã‚µãƒ¼ãƒãƒ¼ã®é€£æºæ§‹æƒ³

* **AIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ**ã¯MCPã®toolsã‚’ç›´æ¥å‘¼ã³å‡ºã™ï¼š

  * `semantic_search`: ä»»æ„è¨€èªã‚¯ã‚¨ãƒªã«å¯¾ã—å¤šè¨€èªæ¤œç´¢
  * `entity_search`: QIDæŒ‡å®šã§ãƒ‹ãƒ¥ãƒ¼ã‚¹æŸå–å¾—
  * `event_timeline`: æœŸé–“ï¼‹ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£æŒ‡å®šã§æ™‚ç³»åˆ—è¿”å´
* **è¿”å´JSON**ã¯ã™ã¹ã¦AIãŒå†æ§‹æˆã§ãã‚‹å½¢ã«è¨­è¨ˆæ¸ˆã¿ï¼ˆtitle, url, entities, evidence spanï¼‰
* **æœ€çµ‚çš„ã«AIãŒå›ç­”ç”Ÿæˆ**ã™ã‚‹éš›ã€evidenceã¨ã—ã¦MCPã‹ã‚‰ã®æ ¹æ‹ ã‚’å¼•ç”¨å¯èƒ½ã«ã™ã‚‹ã€‚

---

## 6. ä»Šå¾Œã®ãƒ¬ãƒãƒ¼ãƒˆã¾ã¨ã‚

* **ç¾çŠ¶**ï¼šMVPæ‰‹å‰ï¼ˆRSS ingestâ†’åŸ‹ã‚è¾¼ã¿â†’æ¤œç´¢ãŒé€šã‚‹ï¼‰ã€‚UIã¯æš«å®šã€‚
* **æ¬¡ã®åˆ°é”ç‚¹**ï¼šã‚¹ãƒ—ãƒªãƒ³ãƒˆ1ã§ã€ŒAIãŒè‡ªåˆ†ã§ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’æ¤œç´¢ã§ãã‚‹ã€çŠ¶æ…‹ã«ã€‚
* **ä¸­æœŸã‚´ãƒ¼ãƒ«**ï¼šã‚¹ãƒ—ãƒªãƒ³ãƒˆ3ã§ã€ŒAIãŒã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ãƒ»ã‚¤ãƒ™ãƒ³ãƒˆå˜ä½ã§ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’è¦ç´„ãƒ»åˆ†æã§ãã‚‹ã€åŸºç›¤å®Œæˆã€‚
* **é•·æœŸã‚´ãƒ¼ãƒ«**ï¼šã‚«ã‚¹ã‚¿ãƒ ã‚½ãƒ¼ã‚¹ãƒ»ãƒ©ãƒ³ã‚­ãƒ³ã‚°æœ€é©åŒ–ãƒ»è©•ä¾¡æŒ‡æ¨™é‹ç”¨ã¾ã§çµ„ã¿è¾¼ã¿ã€å®Ÿç”¨æ°´æº–ã®AIãƒ‹ãƒ¥ãƒ¼ã‚¹é›†ç´„ã€‚

---

ğŸ‘‰ ã“ã®ç¿»è¨³ç‰ˆã‚’ãƒ™ãƒ¼ã‚¹ã«ã€**Day3ä»¥é™ã®é€²è¡Œç®¡ç†ãƒ¬ãƒãƒ¼ãƒˆ**ã¨ã—ã¦ã€ŒMVPé”æˆï¼ˆã‚¹ãƒ—ãƒªãƒ³ãƒˆ1ï¼‰ã€ã¾ã§ã®ã‚¿ã‚¹ã‚¯ã‚’å›ºã‚ã‚‹ã®ãŒæ¬¡ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã§ã™ã€‚

æ¬¡ã«ã€ç§ã‹ã‚‰ã¯ **å®Ÿéš›ã®patchè¨ˆç”»ï¼ˆå·®åˆ†ãƒ•ã‚¡ã‚¤ãƒ«ï¼‰** ã‚’ç”¨æ„ã—ã€`schema_v2.sql`ã‚„`server.py`ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹è¿½åŠ ãƒ»è·é›¢é–¢æ•°çµ±ä¸€ãƒ»ãƒ†ã‚¹ãƒˆæ›´æ–°ã‚’æç¤ºã™ã‚‹ã®ãŒè‰¯ã„ã¨æ€ã„ã¾ã™ã€‚